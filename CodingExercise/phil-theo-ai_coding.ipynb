{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Philosophy and Theory of AI: Coding Exercise\n",
    "\n",
    "To goal of this exercise is to build an AI model (specifically, a neural network) that can classify images of a hand-written digits according to which digit they depict. This is a standard AI exercise. Its point for our course is to concretely see how an AI model is implemented. In doing so, we follow the standard 4-step machine learning pipeline. To do this exercise, simply continue reading through this document. \n",
    "\n",
    "> Whenever you actively have to do something, this is indicated by a gray bar like here. \n",
    "\n",
    "This file is inspired by an [exercise](https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/programming-exercise) (licensed under the Apache License, Version 2.0) in Google's machine learning [crash course](https://developers.google.com/machine-learning/crash-course). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, a comment on how to use *jupyter notebooks* like this one (if you haven't done this before). There are two types of cells: *text cells* like the present one and *code cells* like the next one. Text cells are written in [Markdown](https://en.wikipedia.org/wiki/Markdown) text and code cells are written in [Python](https://en.wikipedia.org/wiki/Python_(programming_language)) code. You can run a cell either by clicking its play button or by pressing `Ctrl` + `Enter` when the cell is highlighted. Running a text cell just renders it ('makes it look like it was intended'), and running a code cell means executing the piece of program that it contains (its output, if any, is then printed below the code). More detailed introductions to jupyter notebook can be found online, e.g., [here](https://www.youtube.com/watch?v=HW29067qVWk).\n",
    "\n",
    "> To practice, run the code cell below. This is just to load the Python packages that we will need later on. You can ignore the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Problem (task conceptualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem that we want our AI model to solve is to classify images. Concretely, given as input a digital image of a hand-written digit, the AI model should output the digit that is depicted on the image. Each image consists of 28x28 gray-scale pixels. Here are some examples (taken from [wikipedia](https://en.wikipedia.org/wiki/MNIST_database)): \n",
    "\n",
    "![Example images from the MNIST database](https://upload.wikimedia.org/wikipedia/commons/f/f7/MnistExamplesModified.png)\n",
    "\n",
    "> To appreciate the difficulty of this problem, take a moment to think about how you would go about building an automatic system that can solve the task. \n",
    "\n",
    "The solution we follow here is to build a neural network. Let's explain how a neural network works. For brevity, this will be quite hand-wavy, but for an excellent detailed video explanation, see [here](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi).\n",
    "\n",
    "As shown in the diagram below (taken from [wikipedia](https://en.wikipedia.org/wiki/Artificial_neural_network)), it consists of a layer of several input neurons (one for each pixel of the image), followed by one (or also several) layers of hidden neurons, and completed by one layer of output neurons (one for each possible digit).\n",
    "\n",
    "![Figure of a neural network](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg)\n",
    "\n",
    "The idea is this: Given an image, we put the gray-scale value of each pixel into its corresponding neuron in the input layer. We call this number the activation of the neuron. Now we propagate this activation through the network as follows: Each connection between neurons has a weight, which modulates how much of the activation can 'flow' through that connection into the next neuron. The activation of that neuron is sum of all the incoming modulated activation (additionally regulated by an activation function). Thus, we get activations of the output neurons. We consider the one with the highest activation: the digit that corresponds to it, is the digit that the network takes the image to depict. We can improve the network's prediction by adjusting the weigts: that will be the training process.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data (collection and preparation) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to collect data: many pairs of an image and the digit it depicts. In practice, this usually is hard work: you first need to collect a wide variety of possible inputs (here the images) and then need human workers (who are called anotators or raters) to label these inputs with the right output (here the depicted digit). Fortunately, since we are doing a standard exercise, this work has already been done for us. The [MNIST database](https://en.wikipedia.org/wiki/MNIST_database) contains many such image-label pairs. (It was created based on a dataset from the US agency NIST, the National Institute of Standards and Technology; to learn more about them, you can watch [this video](https://www.youtube.com/watch?v=esQyYGezS7c).) Conveniently, we can simply download it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the dataset\n",
    "\n",
    "> To download the MNIST dataset, run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us four lists (technically, numpy arrays) called `x_train`, `y_train`, `x_test`, and `y_test`. The 'x-lists' are lists of images and the 'y-lists' are lists of corresponding labels. The 'tain-lists' are used for training the AI model and the 'test-lists' are reserved until the very end: once the AI model is trained, then we test it on this reserved part of the data set. This is *extremely* important: we need to test the AI model on data it has never seen during training. Otherwise we do not get a good estimation on how well the AI model behaves. So never ever mix the test and training set in building your AI model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the dataset\n",
    "\n",
    "Now it is time to actually look at the dataset to get a feel for it. \n",
    "\n",
    "> To look at, say, the 17th datapoint, run the following code. Think about what to make of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The x-value: ', x_train[17])\n",
    "print('The y-value: ', y_train[17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not very helpful yet, right? We do see that the input is an 28x28 array of numbers (a list of 28 lists each of which has 28 entries) between 0 and 255. So each element of the array describes a pixel and the number describes its gray scale between 0 (completely white) and 255 (completely black). And apparently this image depicts an 8.\n",
    "\n",
    "> To better see what is going, we can plot the input using a plotting package for Python. To see it, run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize datapoints\n",
    "\n",
    "Since neural networks work with continuous numbers, it is better to rescale the (whole) numbers between 0 and 255 into (real) numbers between 0 and 1. The mathematical expression for this is to *normalize* the numbers. \n",
    "\n",
    "> To do this, run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_normalized = x_train / 255.0\n",
    "x_test_normalized = x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As a bonus exercise, verify that the numbers are indeed normalized now by adding a new code cell below and printing the 17-th datapoint again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model (architecture, training, and testing) \n",
    "\n",
    "Now that we have both conceptualized our task (which dicates the neural network architecture) and gathered our dataset, we can start building the AI model. This involves three steps: (a) creating a deep neural network, (b) training it, and (c) evaluating its performance. Once they are in place, we can (d) execute these three steps for specific choices for parameters, and then we can (e) optimize the choice of parameter to get better performance. We do these things in turn now. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Creating a deep neural network\n",
    "\n",
    "We define a function that takes one argument (namely, `my_learning_rate`) and outputs a `model` which is the computer implementation of a neural network with the architecture specified in the function definition.\n",
    "\n",
    "> Read through the definition of the function and try to understand as much of the code as possible. It's okay if much of it doesn't make sense, especially on a first reading. For now, the focus is to first get the whole thing running, and then you can come back again to understand more. But try to understand which architecture (how many hidden layers, how many neurons per layer) the neural network has. Look up some terms that you don't know online. (Yes, that is not the most fun thing to do, but it gives an impression of a big part of coding: googling things :-)). Finally, run the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(my_learning_rate):\n",
    "  \"\"\"Create and compile a deep neural net.\"\"\"\n",
    "  \n",
    "  # All models in this course are sequential.\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  # The features are stored in a two-dimensional 28X28 array. \n",
    "  # Flatten that two-dimensional array into a one-dimensional \n",
    "  # 784-element array.\n",
    "  model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "  # Define the first hidden layer.   \n",
    "  model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
    "      \n",
    "  # Define a dropout regularization layer. \n",
    "  model.add(tf.keras.layers.Dropout(rate=0.2))\n",
    "\n",
    "  # Define the output layer. The units parameter is set to 10 because\n",
    "  # the model must choose among 10 possible output values (representing\n",
    "  # the digits from 0 to 9, inclusive).\n",
    "  #\n",
    "  # Don't change this layer.\n",
    "  model.add(tf.keras.layers.Dense(units=10, activation='softmax'))     \n",
    "                           \n",
    "  # Construct the layers into a model that TensorFlow can execute.  \n",
    "  # Notice that the loss function for multi-class classification\n",
    "  # is different than the loss function for binary classification.  \n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=my_learning_rate),\n",
    "                loss=\"sparse_categorical_crossentropy\",\n",
    "                metrics=['accuracy'])\n",
    "  \n",
    "  return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Training a deep neural network\n",
    "\n",
    "Next we define a function that takes as input a model (the output of the previous function), a dataset, and several arguments, then trains the model with that data, and finally outputs a history of the training process.\n",
    "\n",
    "> With the same spirit as in the previsous exercise, read through the definition of the function and try to understand as much of the code as possible. Then, run the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_features, train_label, epochs,\n",
    "                batch_size=None, validation_split=0.1):\n",
    "  \"\"\"Train the model by feeding it data.\"\"\"\n",
    "\n",
    "  history = model.fit(x=train_features, y=train_label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=True, \n",
    "                      validation_split=validation_split)\n",
    " \n",
    "  # To track the progression of training, gather a snapshot\n",
    "  # of the model's metrics at each epoch. \n",
    "  epochs = history.epoch\n",
    "  hist = pd.DataFrame(history.history)\n",
    "\n",
    "  return epochs, hist    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Evaluate a deep neural network\n",
    "\n",
    "Finally, we define a function to evalute the (trained) model. It takes as input the history of the training process and a list of evaluation criteria (we will only use accuracy), and then procudes a plot of how the accuracy improves over the course of training.\n",
    "\n",
    "> You don't need to focus much on the details of this function. But do run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curve(epochs, hist, list_of_metrics):\n",
    "  \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n",
    "  # list_of_metrics should be one of the names shown in:\n",
    "  # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#define_the_model_and_metrics  \n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Value\")\n",
    "\n",
    "  for m in list_of_metrics:\n",
    "    x = hist[m]\n",
    "    plt.plot(epochs[1:], x[1:], label=m)\n",
    "\n",
    "  plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Put everything together\n",
    "\n",
    "Finally, for specific choices of parameters, we execute the above functions. This can take some time (depending on the size of the model, computing speed, etc., but roughly in the order of several seconds to minutes).\n",
    "\n",
    "> Execute the cell and make sure you understand the output. In particular, the very last line of the output tells you how well the trained model performed on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.003\n",
    "epochs = 50\n",
    "batch_size = 4000\n",
    "validation_split = 0.2\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate)\n",
    "\n",
    "# Train the model on the normalized training set.\n",
    "epochs, hist = train_model(my_model, x_train_normalized, y_train, \n",
    "                           epochs, batch_size, validation_split)\n",
    "\n",
    "# Plot a graph of the metric vs. epochs.\n",
    "list_of_metrics_to_plot = ['accuracy']\n",
    "plot_curve(epochs, hist, list_of_metrics_to_plot)\n",
    "\n",
    "# Evaluate against the test set.\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x=x_test_normalized, y=y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Optimize\n",
    "\n",
    "Now we get to the main exercise. As things have been defined so far, you should achieve a test accuracy of around 96%. That is already pretty impressive, don't you think? But now we want to see if we can change the parameters of the AI model to achieve an even better performance.\n",
    "\n",
    "> Systematically adjust the following parameters and observe how the performance is changing: \n",
    ">    * number of hidden layers\n",
    ">    * number of nodes in each layer\n",
    ">    * dropout regularization rate\n",
    "> \n",
    "> Describe which relationships you discover. And do you manage to reach at least 98% accuracy on the test set?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Deployment (distribution shifts, explanations, etc.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next sets in the development of an AI model for the real world would be to delpoy it. (Here maybe a system that recognizes handwriten phone numbers and outputs them in digital format.) For that we would need to study how robust it is to distributions shifts: For example, if it still performs well in countries (e.g., Germany) where, unlike the US, the digit $1$ is hand-written not just as a vertical bar, but as a vertical bar with a little 'flag' to the left. Is this shift in distribution of the input images causing a drop in accuracy or is the model robust to this. But for the purpose of this exercise, we will not do this here.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phil-theo-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
